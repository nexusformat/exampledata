
import os
import pkg_resources
import h5py
import glob
import datetime
import numpy as np
from lxml import etree
import bs4

readme_string = """

        Autogenerated using version [%s] of the NEXUS definitions.

The file generated by this script is intended to give the user an example of a 
file that conforms to the [%s] application definition such that it will pass 
validation with cnxvalidate. There are some attributes that have been added for 
informational purposes to help the user know what is a required field or group, 
the informational only attributes have the prefix 'EX_', for example if a group 
or field is required there will exist an attribute called EX_required that will 
be set to true, if it is not required it will be set to false. Where the definition 
specifies a doc string for a field or group that doc string will also be included 
as a string attribute for that field or group called EX_doc. 
 
 Fields or groups in the generated file that have 'untitled_' as a prefix are intended 
 to indicate to the user that in the file they produce they need to replace the name of 
 that field with the name they want. There is a 1 to 1 relationship between the generated 
 file and the release version of the NEXUS application definitions that the user has 
 installed. If the user updates their application definitions then they should also 
 run nxdl_to_hdf5.py to regenerate the files. 
 
 The auto generated files are placed in:
 <nexus definition repo>/exampledata/autogenerated_examples/nxdl/applications.
 
 https://github.com/nexusformat/exampledata
 
 """

# EXAMPLE_CODE = ['import numpy as np',
#     'import datetime',
#     'import h5py as h5',
#     'import os',
#     'root.attrs["'file_name'"] = os.path.abspath(''example.h5'')',
# root.attrs['file_time'] = datetime.datetime.now().isoformat(),
# root.attrs['h5py_version'] = h5.version.version,
# root.attrs['HDF5_Version'] = h5.version.hdf5_version,
#     ]

"""
nexusformat example
from nexusformat.nexus import *

Qh = NXfield(np.linspace(0, 5.0, 51), name='Qh')
Qk = NXfield(np.linspace(0, 4.0, 41), name='Qk')
H, K = np.meshgrid(Qh, Qk)
data = NXfield(np.exp(-((H-2.5)**2 + (K-2)**2)), name='data')

root = NXroot()
root['entry'] = NXentry()

root['entry/sample'] = NXsample()
root['entry/sample/temperature'] = NXfield(40.0, units='K')
root['entry/sample/mass'] = NXfield(10.0, units='g')

root['instrument'] = NXinstrument(NXmonochromator())
root['instrument/monochromator/energy'] = NXfield(87.1, units='keV')

root['entry/data'] = NXdata(data, (Qk, Qh))
root['entry/data'].set_default()

root['entry/title'] = 'Example NeXus Data (nexusformat)'

root.save('example.nxs', 'w')

"""
# a dict of nx types, their default values and doc strings
nx_unit_types = {
 'NX_ANGLE' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of angle, example(s): m'},
 'NX_ANY' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units for things like logs that aren’t picky on units'},
 'NX_AREA' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of area, example(s): m^2 | barns'},
 'NX_CHARGE' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of electrical charge, example(s): c'},
 'NX_CROSS_SECTION' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of area (alias of NX_AREA, example(s): barn'},
 'NX_CURRENT' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of electrical current, example(s): A'},
 'NX_DIMENSIONLESS' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units for fields where the units cancel out. NOTE: not the same as NX_UNITLESS, example(s): m/m'},
 'NX_EMITTANCE' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of emittance (length * angle) of a radiation source, example(s): nm*rad'},
 'NX_ENERGY' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of energy, example(s): J | keV'},
 'NX_FLUX' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of flux, example(s): 1/s/cm^2'},
 'NX_FREQUENCY' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of frequency, example(s): Hz'},
 'NX_LENGTH' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of length, example(s): m'},
 'NX_MASS' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of mass, example(s): g'},
 'NX_MASS_DENSITY' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of mass density, example(s): g/cm^3'},
 'NX_MOLECULAR_WEIGHT' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of molecular weight, example(s): g/mol'},
 'NX_PERIOD' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of time, period of pulsed source alias to NX_TIME, example(s): us'},
 'NX_PER_AREA' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of 1/length^2, example(s): 1/m^2'},
 'NX_PER_LENGTH' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of 1/length, example(s): 1/m'},
 'NX_POWER' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of power, example(s): W'},
 'NX_PRESSURE' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of pressure, example(s): Pa'},
 'NX_PULSES' : {'val': 1, 'type': 'NX_INT', 'doc':'units of clock pulses (alias to NX_NUMBER'},
 'NX_SCATTERING_LENGTH_DENSITY' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of scattering length density, example(s): m/m^3'},
 'NX_SOLID_ANGLE' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of solid angle, example(s): sr | steradian'},
 'NX_TEMPERATURE' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of temperature, example(s): K'},
 'NX_TIME' : {'val': '1.0', 'type': 'NX_FLOAT', 'doc':'units of time, example(seconds): 12.5'},
 'NX_TIME_OF_FLIGHT' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of (neutron) time of flight alias to NX_TIME, example(s): s'},
 'NX_TRANSFORMATION' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of the specified transformation could be any of these: NX_LENGTH, NX_ANGLE, or NX_UNITLESS. There will be one or more transformations defined by one or more fields for each transformation. The units type NX_TRANSFORMATION designates the particular axis generating a transformation (e.g. a rotation axis or a translation axis or a general axis. NX_TRANSFORMATION designates the units will be appropriate to the type of transformation, indicated in the NXtransformations base class by the transformation_type value: NX_LENGTH for translation, NX_ANGLE for rotation, NX_UNITLESS for axes for which no transformation type is specified.'},
 'NX_UNITLESS' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'for fields that don’t have a unit (e.g. hkl) so that they don’t inherit the wrong units (NOTE: not the same as NX_DIMENSIONLESS, example(s): '},
 'NX_VOLTAGE' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of voltage, example(s): V'},
 'NX_VOLUME' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of volume, example(s): m3'},
 'NX_WAVELENGTH' : {'val': 1.0, 'type': 'NX_FLOAT',  'doc':'units of wavelength, example(s): angstrom'},
 'NX_WAVENUMBER' : {'val': 1.0, 'type': 'NX_NUMBER',  'doc':'units of wavenumber or Q, example(s): 1/nm | 1/angstrom'}
}

h5py_script_lst = []
nxsfrmt_script_lst = []
CUR_GRP_DTYPE = ''

def _string_attr(nxgrp, name, sdata, do_print=True):
    '''
    used to create string attributes
    '''
    sdata = sdata.replace('\t',' ')
    sdata = sdata.replace('\n', ' ')
    if (nxgrp is None):
        return
    nxgrp.attrs[name] = sdata

    if do_print:
        sdata = sdata.replace(__file__.replace('/nxdl_to_hdf5.py', ''), '')
        sdata = sdata.replace('/..', '..')
        if sdata.find('.hdf5') > -1:
            #just use the file name
            h5py_script_lst.append('root[\'%s\'].attrs[\'%s\'] = \'%s\'' % (nxgrp.name, name, sdata.split('/')[-1]))
        else:
            h5py_script_lst.append('root[\'%s\'].attrs[\'%s\'] = \'%s\'' % (nxgrp.name, name, sdata))
        #print('root[\'%s\'].attrs[\'%s\'] = \'%s\'' % (nxgrp.name, name, sdata.replace('\'', '"')))

def _num_attr(nxgrp, name, data, do_print=True):
    '''
    used to create string attributes
    '''
    if (nxgrp is None):
        return
    nxgrp.attrs[name] = data

    if do_print:
        h5py_script_lst.append('root[\'%s\'].attrs[\'%s\'] = \'%s\'' % (nxgrp.name, name, str(data)))
        #print('root[\'%s\'].attrs[\'%s\'] = \'%s\'' % (nxgrp.name, name, sdata.replace('\'', '"')))

def _list_attr(nxgrp, name, lstdata, do_print=True):
    '''
    used to create list attributes
    '''
    #print(nxgrp, name, lstdata)
    if (nxgrp is None):
        return
    if (name in list(nxgrp.attrs.keys())):
        nxgrp.attrs[name][()] = lstdata
    else:
        nxgrp.attrs[name] = lstdata


def print_list_attr(nxgrp, name, lstdata, do_print=True):
    if do_print:
        h5py_script_lst.append(' ')
        #h5py_script_lst.append('root[\'%s\'].attrs[\'%s\'] = \'%s\'' % (nxgrp.name, name, str(lstdata)))
        #h5py_script_lst.append('# the allowable values for root[\'%s\'].attrs[\'%s\'] are: \'%s\'' % (nxgrp.name, name, str(lstdata)))
        h5py_script_lst.append(
            '# Valid enumeration values for root[\'%s\'][\'%s\'] are: ' % (nxgrp.name, name))
        for i in lstdata:
            h5py_script_lst.append('#\t %s' % i)


def check_for_duplicate_grp_names(nxgrp, name, nxdata_type):
    # if name of group already exists then increment name by one until it is unique
    i = 1
    _name = name
    while name in nxgrp.keys() and i < 100:
        name = _name + '_%d' % i
        i += 1
        break
    return(name)


def _group(nxgrp, name, nxdata_type, dct={}):
    '''
    create a group
    '''
    if (name == ''):
        return
    # #if name of group already exists then increment name by one until it is unique
    # i = 1
    # _name = name
    # while name in nxgrp.keys() and i < 100:
    #     name = _name + '_%d' % i
    #     i += 1
    name = check_for_duplicate_grp_names(nxgrp, name, nxdata_type)
    grp = nxgrp.create_group(name)

    # #update the group name if it changed as NX canSAS specifies 2 NXdata groups at same level but only one is required
    dct['abspath'] = grp.name
    print_script_group(dct)

    _string_attr(grp, 'NX_class', nxdata_type)
    return (grp)

def _dataset(nxgrp, name, data, nxdata_type, nx_units='', dset={}, do_print=True):
    '''
    create a dataset, apply compression if the data is an array
    '''
    # grp = nxgrp.create_dataset(name=name, data=data, maxshape=None)
    #strip any units off of nxdata_type
    _dt = nxdata_type.split('-')
    nxdata_type = _dt[0]

    if type(data) == np.chararray:
        grp = nxgrp.create_dataset(name=name, data=data, maxshape=None)

        data = str(data.astype(str))
        if data.find('\' ') > -1:
            data = data.replace('\' ', '\' , ')
        data = data.encode("ascii", "ignore")
        h5py_script_lst.append(' ')
        h5py_script_lst.append('root[\'{}\'][\'{}\'] = {}'.format(nxgrp.name, name, data))

    elif (type(data) == np.ndarray):
        #print('creating np.ndarray dataset [%s]' % name)
        grp = nxgrp.create_dataset(name=name, data=data, maxshape=None, compression="gzip")

        if nxdata_type == 'NX_FLOAT':
            #s = str(data.astype(float))
            s = str(data.astype(float)).replace('\n', ',')
        elif nxdata_type in ['NX_INT', 'NX_NUMBER', 'NX_POSINT', 'NX_UINT']:
            s = str(data.astype(int)).replace('\n', ',')
        else:
            print('_dataset: UNSUPPORTED ndarray TYPE %s' % nxdata_type)

        if s.find('. ') > -1:
            data = s.replace('. ', '. , ').replace('\n', ',')
        h5py_script_lst.append(' ')
        h5py_script_lst.append('root[\'%s\'].create_dataset(name=\'%s\', data=%s, maxshape=None, compression="gzip")' % (nxgrp.name, name, str(data)))

            #h5py_script_lst.append('root[\'{}\'][\'{}\'] = {}'.format(nxgrp.name, name, data))

    else:
        #print('name[%s] data[%s]' % (name, data))
        if(data is None):
            data = ''
        #print('creating dataset [%s]' % name)
        grp = nxgrp.create_dataset(name=name, data=data, maxshape=None)
        s_data = str(data)
        if type(data) is str:
            if do_print:
                h5py_script_lst.append(' ')
                h5py_script_lst.append('root[\'%s\'].create_dataset(name=\'%s\', data=\'%s\', maxshape=None)' % (nxgrp.name, name, s_data))
                #h5py_script_lst.append('root[\'%s\'][\'%s\'] = \'%s\'' % (nxgrp.name, name, str(data)))
                #h5py_script_lst.append('root[\'{}\'][\'{}\'] = \'{}\''.format(nxgrp.name, name, data))

                #print('root[\'%s\'].create_dataset(name=\'%s\', data=\'%s\', maxshape=None)' % (nxgrp.name, name, str(data)))
                #nxsfrmt_script_lst.append('root[\'%s\'] = NXfield(\'%s\', \'%s\')' % (nxgrp.name, name, data))
                nxsfrmt_script_lst.append('root[\'{}\'] = NXfield(\'{}\', \'{}\')'.format(nxgrp.name, name, data))
        else:
            if do_print:
                h5py_script_lst.append(' ')
                if s_data.find('array') > -1:
                    # make sure it reads correctly as a numpy array
                    s_data = s_data.replace('array', 'np.array')
                h5py_script_lst.append('root[\'%s\'].create_dataset(name=\'%s\', data=%s, maxshape=None)' % (nxgrp.name, name, s_data))
                #h5py_script_lst.append('root[\'%s\'][\'%s\'] = %s' % (nxgrp.name, name, str(data)))
                #h5py_script_lst.append('root[\'{}\'][\'{}\'] = {}'.format(nxgrp.name, name, data))

                #print(
                #    'root[\'%s\'].create_dataset(name=\'%s\', data=%s, maxshape=None)' % (nxgrp.name, name, str(data)))
                nxsfrmt_script_lst.append('root[\'%s\'] = NXfield(\'%s\', \'%s\')' % (nxgrp.name, name, s_data))

    # grp_name = get_last_name_from_abspath(dct['abspath'])
    # print('root[\'%s\'] = h5py.create_group(\'%s\')' % (get_parent_path(dct['abspath']), grp_name))

    _string_attr(grp, 'type', nxdata_type, do_print)

    if len(nx_units) > 0:
        if (type(nx_units) is dict):
            _string_attr(grp, 'units', nx_units['units'], do_print)
        else:
            _string_attr(grp, 'units', nx_units, do_print)

    if ('doc' in list(dset.keys())):
        _string_attr(grp, 'doc', dset['doc'], do_print)


    return (grp)

def get_abspath(a):
    '''
    walking the parents of an xml attribute, return a list of the parent names
    '''
    #s = '/'
    l = None
    k = None
    parent = a.getparent()
    if(parent is not None):
        if('name' in a.attrib.keys()):
            k = 'name'
        elif('type' in a.attrib.keys()):
            k = 'type'
        l = get_abspath(parent)
    if (l is None):
        l = []
    if(k):
        l.append(a.attrib[k])

    if(len(l) > 0):
        if(None in l):
            l.remove(None)
        return(l)
    else:
        return([])

def abspath_lst_to_str(l):
    '''
    assemble a path string from list of names
    '''
    if l is None:
        print()
    s = '/'
    for n in l:
        s += '%s/' % fix_nx_name(n)
    return(s)

def get_children_list_of_lists(e):
    '''
    create a list of dicts for each item
    '''
    pth_lst = get_abspath(e)
    abspath = abspath_lst_to_str(pth_lst)
    t = e.getroottree()
    e_txt = None
    if e.text is not None:
        e_txt = e.text.replace('\n',' ')
    lst = [{'ptype': e.tag, 'xpath': t.getpath(e), 'abspath': abspath, 'attrib': dict(e.attrib), 'doc': e_txt}]
    children = e.getchildren()
    for ch in children:
        if(len(ch.getchildren()) > 0):
            lst.append(get_children_list_of_lists(ch))
        else:
            t = ch.getroottree()
            ch_pth_lst = get_abspath(ch)
            ch_abspath = abspath_lst_to_str(ch_pth_lst)
            ptype = ch.tag
            ch_txt = None
            if ch.text is not None:
                ch_txt = ch.text.replace('\n', ' ')
            if(ptype):
                lst.append({'ptype': ch.tag, 'xpath': t.getpath(ch), 'abspath': ch_abspath,
                            'attrib': dict(ch.attrib), 'doc': ch_txt})
    return(lst)

def get_children(e):
    '''
    get the children of an element return as a list
    '''
    lists = get_children_list_of_lists(e)
    l = list(flatten(lists))
    return(l)

def flatten(lst):
    for el in lst:
        if isinstance(el, list):
            # recurse
            yield from flatten(el)
        else:
            # generate
            yield el

def get_type(d):
    known_int_types = []
    known_char_types = []
    if 'type' not in d['attrib'].keys():
        if d['attrib']['name'] in known_int_types:
            return('NX_INT')

        if 'units' in d['attrib'].keys():
            #return a concatenation of the default for no specified type 'NX_CHAR' and '-<units>
            return ('NX_CHAR-%s' % d['attrib']['units'])
        #return ('NX_CHAR')
        return ('')
    else:
        return(d['attrib']['type'])

def get_parent_path(pth):
    pth2 = pth.split('/')
    for p in pth2:
        if(p == ''):
            pth2.remove(p)
    p = abspath_lst_to_str(pth2[0:-1])
    return(p)

def get_last_name_from_abspath(pth):
    pth2 = pth.split('/')
    for p in pth2:
        if(p == ''):
            pth2.remove(p)
    return(pth2[-1])


def get_min_occurs(d, category):
    '''
        a minOccurs that is not defined has different defaults depending on the type or 'category' of definition it is.
        category="base" definitions the default when there is no minOccurs defined is that they are 'optional'
        category="application" definitions the default when there is no minOccurs defined is that they are 'EX_required'
        category="contributed" definitions the default when there is no minOccurs defined is that they are 'EX_required'
    '''
    valid_categories = ['base', 'application', 'contributed']
    if category not in valid_categories:
        print('Error: invalid definition category [%s]' % (category))
        print('\t must be one of ', valid_categories)
        exit()
    if ('minOccurs' in d['attrib'].keys()):
        min_occurs = int(d['attrib']['minOccurs'])
        if (min_occurs > 0):
            reqd = 'true'
        else:
            reqd = 'false'
    else:
        if(category.find('application') > -1):
            #default for application definitions if no minOccurs specified
            reqd = 'true'
        else:
            # default for all other definitions if no minOccurs specified
            reqd = 'false'
    return(reqd)

def get_doc(d):
    doc = None
    if('doc' in d.keys()):
        if(type(d['doc']) is str):
            if(len(d['doc'].strip()) > 1):
                doc = d['doc']
    return(doc)

def sanity_check_dimensions(dct, sym_dct={}, show_warnings=True):
    '''
     for every dimensions spec there should be a 'rank' specified and there should be at least the same number
     of accompnying dim sections as the value specified in 'rank'
    '''

    if(type(dct) is dict):
        if('attrib' in dct.keys()):
            if('rank' in dct['attrib'].keys()):
                if(has_numbers(dct['attrib']['rank'])):
                    rank = int(dct['attrib']['rank'])
                elif has_expression(dct['attrib']['rank']):
                    rank = 1
                    if show_warnings:
                        print(
                            '\t\tNote: rank designation is using an expression, expressions are currently not supported for generation, the value of 1 will be used')
                        print('\t\t The expression used in the definition is: [%s]' % (dct['attrib']['rank']))
                else:
                    if show_warnings:
                        print(
                            '\t\tError: rank designation is using a symbol that has not been defined in Symbols table or it is a comment')
                        print('\t\t [%s]' % (dct['attrib']['rank']))
                    return(False)

                if(rank <= len(dct['attrib']['dim'])):
                    return(True)
                else:
                    if show_warnings:
                        print('\t\tError: Incorrect number of dim sections for this dimensions specification, should be at least [%d] found [%d]' %  (rank, len(dct['attrib']['dim'])))
                        print('\t\t[%s]' % dct['abspath'])
                    return (False)
            else:
                #print('\t\tError: dimensions must specify a rank, this does not >', dct)
                return(False)
        else:
            return(False)
    else:
        return(False)

def get_nx_data_by_type(nx_type, dimensions=None, sym_dct={}):
    '''
    using the passed in type, return some sample data of the correct type, dimension and size if an array
    '''
    errors = False
    data = None
    use_dims = False
    if( sanity_check_dimensions(dimensions, sym_dct, show_warnings=False)):
        if (dimensions and 'attrib' in dimensions.keys() and len(dimensions['attrib']['dim']) > 0):
            dimensions = dimensions['attrib']
            arr = []
            #first create an array of rank dimensions
            if('rank' not in dimensions.keys()):
                if ('value' in dimensions.keys()):
                    val = int(dimensions['value'])
                    arr.append(val)
                else:
                    arr.append(1)
            else:
                #check if symbol was used in defining a rank
                if(has_numbers(dimensions['rank'])):
                    if 'value' in dimensions.keys():
                        if not has_numbers(dimensions['value']):
                            if dimensions['value'] not in sym_dct.keys():
                                print('\t-Most likely this [%s] should be a symbol but has not been defined in the Symbols table as one' % dimensions['value'])
                            rank = 1
                            sym_dct[dimensions['value']] = rank
                        else:
                            rank = int(dimensions['value'])
                    else:
                        rank = 1
                    rank = int(dimensions['rank'])
                    for r in range(0, rank):
                        #verify length of dim as teh number of dim entries must equal rank size
                        if (len(dimensions['dim']) < rank):
                            print('\t-Invalid NXDL file, the number of DIM entries must equal the size of specified rank [%d]' % rank)
                            errors = True
                            return(None)
                        # check if symbol was used in defining a dim value
                        _keys = dimensions['dim'][r].keys()
                        #if('value' not in dimensions['dim'][r].keys()):
                        if 'value' not in _keys:
                            print('\t\tError, there is no [value] key in dim specification' , dimensions['dim'][r])
                            errors = True
                            return (None)
                        if not has_numbers(str(dimensions['dim'][r]['value'])):
                            if dimensions['dim'][r]['value'] not in sym_dct.keys():
                                print('\t-Most likely this [%s] should be a symbol but has not been entered in the definition as one' % dimensions['dim'][r]['value'])
                                val = 1
                                sym_dct[dimensions['dim'][r]['value']] = val

                            else:
                                #substitute the value for the one defined for the symbol
                                val = int(sym_dct[dimensions['dim'][r]['value']])
                        else:
                            val = int(dimensions['dim'][r]['value'])

                        arr.append(val)
                        use_dims = True
                else:
                    arr.append(1)
            if use_dims:
                if(nx_type.find('NX_INT') > -1):
                    data = np.ones(tuple(arr), dtype=np.int)
                elif(nx_type.find('NX_NUMBER') > -1):
                    data = np.ones(tuple(arr), dtype=np.int)
                elif(nx_type.find('NX_FLOAT') > -1):
                    data = np.ones(tuple(arr), dtype=np.float)
                elif (nx_type.find('NX_CHAR') > -1):
                    data = np.chararray(tuple(arr))
                    data[:] = '!some char data!'

    units = get_units(nx_type)
    # if units in nx_unit_types.keys():
    #     unit_dct = nx_unit_types[units]
    #if there are units specified return a data suitable
    if units.find('NX_WAVENUMBER') > -1:
        if (use_dims):
            return (1.0)
        else:
            return (1.0)
    elif(nx_type.find('NX_CHAR') > -1):
        if(data is None):
            return('!some char data!')
        else:
            return(data)
    elif(nx_type.find('NX_FLOAT') > -1):
        if (use_dims):
            return(data)
        else:
            return(1.0)
    elif (nx_type.find('NX_INT') > -1):
        if (use_dims):
            if(type(data) is np.ndarray):
                #convert this array into a list
                data = list(data)
            return (data)
        else:
            return (1)
    elif (nx_type.find('NX_BOOLEAN') > -1):
        #return False, NX_BOOLEAN's are int8's
        return (np.int8(0))

    elif (nx_type.find('NX_DATE_TIME') > -1):
        return(make_timestamp_now())
    elif (nx_type.find('NX_NUMBER') > -1):
        if (use_dims):
            return (data)
        else:
            return (1.0)
    elif (nx_type.find('NX_POSINT') > -1):
        if (use_dims):
            return (data)
        else:
            return (1)
    elif (nx_type.find('NX_UINT') > -1):
        if (use_dims):
            return (data)
        else:
            return (1)


def get_units(nx_type):
    # check to see if this nx_type contains the units as well
    # ex: NX_CHAR-NX_WAVENUMBER
    units = nx_type.split('-')
    if len(units) == 2:
        return(units[1])
    else:
        return('')

def make_timestamp_now():
    """
    create a ISO 8601 formatted time string for the current time and return it
    """
    t = datetime.datetime.now().isoformat()
    return (t)

def get_entry(nf):
    '''
    return the name of the entry in the file
    '''
    keys = list(nf.keys())
    s1 = 'entry'
    for k in keys:
        if s1.casefold() == k.casefold():
            return(nf[k], k)
        elif k.casefold().find(s1.casefold()) > -1:
            return (nf[k], k)
        else:
            pass
    return(nf[k], s1)

def get_NXdata_nm(nf):
    '''
    find the first NXdata group and return its name
    '''
    entry_grp, entry_nm = get_entry(nf)
    keys = list(entry_grp.keys())
    s1 = 'NXdata'
    for k in keys:
        if 'NX_class' in entry_grp[k].attrs.keys():
            if s1.casefold() == entry_grp[k].attrs['NX_class'].casefold():
                return(entry_grp[k], k)
    return(None, None)

def get_all_paths_in_hdf5(nf):
    list(nf.keys())
    list(nf.values())
    members = []
    nf.visit(members.append)
    return(members)

def get_NXdataset_nm(nxdata_grp):
    keys = list(nxdata_grp.keys())
    #remove the most likely keys that we do not want for sure
    keys = trim_dset_list(keys)
    res = None
    for k in keys:
        if(k.casefold() == 'data'):
            return(k)
    if(len(keys) > 0):
        #just return the first one for now
        return(keys[0])
    else:
        return(None)

def trim_dset_list(dset_lst):
    rem_lst = ['energy', 'sample_x', 'sample_y']
    lst = []
    for d in dset_lst:
        if(d.casefold() not in rem_lst):
            lst.append(d)
    return(lst)

def has_numbers(inputString):
    #make sure it is a string
    inputString = str(inputString)
    return all([c.isdigit() or c == '.' for c in inputString])

def has_expression(inputString):
    #make sure it is a string
    inputString = str(inputString)
    for op in ['+','-','*','/']:
        if inputString.__contains__(op):
            return(True)
    return False

def get_symbols(fname):
    '''# print(root.xpath("//article[@type='news']/content/text()"))
    # print(root.xpath("//article"))
    '''
    from bs4 import BeautifulSoup
    infile = open(fname, "r")
    contents = infile.read()
    infile.close()
    soup = BeautifulSoup(contents, 'xml')
    symbols = soup.find_all('symbol')
    slst = []
    for symbol in symbols:
        #print(symbol)
        slst.append({'name': symbol.get('name'), 'doc': symbol.get_text(), 'value': 1})

    return(slst)


def process_symbols(soup, sym_args_dct={}):
    '''
    this function pulls the symbols if any are defined and then substitutes those symbols for ones defined by the user
    in usr_args (if the user passed them in on the command line), if no user args define values for each symbol the
    symbol will be assigned the default value of 1
    '''
    symbols = soup.find_all('symbol')
    slst = []
    sym_dct = {}
    for symbol in symbols:
        sym_nm = symbol.get('name')
        val = 1
        doc = symbol.get_text()
        #print(symbol)
        if(sym_nm in sym_args_dct.keys()):
            #use the user passed in value for this symbol
            val = sym_args_dct[sym_nm]
            print('\tsymbol [%s] will use the value [%s] passed in by user' % (sym_nm, str(val)))
        else:
            if(sym_nm == 'dataRank'):
                print('\tsymbol [%s] was not defined and passed in in the usr_args, so it will be auto calculated at every occurance' % (sym_nm))
            else:
                print('\tsymbol [%s] was not defined and passed in in the usr_args, so using default value of 1 for [%s]' % (sym_nm,sym_nm))

        if(type(val) is dict):
            #if users passed in symbols from the command line these will  be in a dict
            val = val['value']

        slst.append({'name': sym_nm, 'doc': doc, 'value': val})
        sym_dct[sym_nm] = {'doc': doc, 'value': val}

    #now walk through each dimension definition and perform symbol substitutions to the read in xml doc where applicable
    # get all dimensions fields where rank is defined using keyword 'dataRank'
    # drank_tags_lst = soup.select('[rank^="dataRank"]')
    drank_tags_lst = soup.find_all('dimensions')
    for dimns in drank_tags_lst:
        max_index = 1
        #first check attrs of this dimensions field for symbols used for values
        cntnts = dimns.contents
        for cntnt_attr in cntnts:
            if(not type(cntnt_attr) == bs4.element.Tag):
                continue
            if 'value' in cntnt_attr.attrs.keys():
                if (cntnt_attr.attrs['value'] in sym_dct.keys()):
                    cntnt_attr.attrs['value'] = sym_dct[ cntnt_attr.attrs['value'] ]['value']

        if 'rank' in dimns.attrs.keys():
            dim_lst = dimns.find_all('dim')
            for d in dim_lst:
                reqd = True
                if(d['index'].isdigit()):
                    #d is a dimensions dict
                    idx = int(d['index'])

                    if('value' not in d.attrs.keys()):
                        print('\t\tError, there is no [value] key in dim specification', d.attrs)
                        continue
                    if(d.attrs['value'] in sym_dct.keys()):
                        # # dimension is required if not specified or if set to true (will it ever be explicitly?)
                        # the definition specifies a symbol as the value for this dimension so substitute it
                        d.attrs['value'] = sym_dct[ d.attrs['value'] ]['value']
                    # dimension is required if not specified or if set to true (will it ever be explicitly?)
                    if 'EX_required' in d.attrs.keys():
                        if d.attrs['EX_required'] == 'false':
                            reqd = False
                    #record max_index so we can set 'dataRank' if it is used
                    if reqd and (idx > max_index):
                        max_index = idx

            #now (if it is specified) substitute max_index for 'dataRank' keyword
            if('rank' in dimns.attrs.keys()):
                if dimns.attrs['rank'] == 'dataRank':
                    dimns.attrs['rank'] = str(max_index)

    # all symbol substitutions have been made in the 'soup'
    xml = soup.prettify(encoding='utf-8')
    xml = xml.decode('utf-8')
    return(xml, sym_dct)


def get_xml_paths(fname, sym_args_dct={}, report_symbols_only=False):
    '''
    takes the path to teh nxdl.xml file and returns a dict of element category lists of the entire structure
    '''
    if(not os.path.exists(fname)):
        print('XML file [%s] does not exist' % fname)
        return

    infile = open(fname, "r")
    contents = infile.read()
    infile.close()
    contents = contents.replace('xmlns="http://definition.nexusformat.org/nxdl/3.1"','')
    contents = contents.replace('xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n', '')
    contents = contents.replace('xsi:schemaLocation="http://definition.nexusformat.org/nxdl/3.1 ../nxdl.xsd"', '')
    soup = bs4.BeautifulSoup(contents, 'xml')
    # syms = get_symbols(fname)
    xml, syms = process_symbols(soup, sym_args_dct=sym_args_dct)
    if(report_symbols_only):
        exit(0)

    ldct = {}
    dct = {}
    docs = []
    xml = xml.replace('encoding="utf-8"', '')
    xxml = etree.XML(xml)
    #xxml = etree.parse(fname).getroot()
    tree = etree.ElementTree(xxml)
    root = tree.getroot()

    def_dir = root.get('category')
    dct['category'] = def_dir
    dct['doc'] = ''
    ch = root.getchildren()

    #confirm that an NXentry group exists, if not then bail
    entry = soup.find(type='NXentry')
    if(not entry):
        return(None, None)

    for c in ch:
        if(c.tag == 'doc'):
            dct['doc'] = c.text
        elif(c.tag == 'group'):
            entry_grp = c
            break

    #skip_lst = ['doc']
    skip_lst = []
    ch_lst = get_children(entry_grp)
    if (len(ch_lst) > 0):
        for ch_dct in ch_lst:
            if ch_dct['ptype'] not in skip_lst:
                hsh = hash(ch_dct['xpath'])
                #print(ch_dct)
                if hsh not in ldct.keys():
                    #print(ch_dct['ptype'] + ch_dct['abspath'])
                    if ch_dct['ptype'] not in dct.keys():
                        dct[ch_dct['ptype']] = []

                    if ch_dct['ptype'] == 'doc':
                        #here using the abspath find the nearest field or group that this doc belongs to and assign it
                        #dct[ch_dct['ptype']] = []
                        docs.append(ch_dct)

                    else:
                        dct[ch_dct['ptype']].append(
                            {'abspath': ch_dct['abspath'], 'xpath': ch_dct['xpath'], 'attrib': dict(ch_dct['attrib']),
                             'doc': ch_dct['doc']})
                        ldct[hsh] = 1

    #pprint.pprint(dct)
    return (dct, syms, docs)

def get_parent_group(nf, ppath):

    if (ppath == ''):
        pgrp = nf
    else:
        if ppath not in nf:
            print('\t\tError: why doesnt [%s] exist?' % ppath)
        else:
            pgrp = nf[ppath]
    return(pgrp)

def get_enums(abspath, dct):
    '''
    for a given parent path, return all enumerations as a list
    '''
    l = []
    for d in dct['item']:
        if(abspath == d['abspath']):
            l.append(d['attrib']['value'])
    return(l)

def fix_nx_name(nm):
    '''
    it appers to be default that if no name attribute was passed then the NX class name is used in upper case sans 'NX'
    '''
    if nm.find('NX') > -1:
        #remove NX and make it upper case
        # nm = nm.replace('NX','').upper()
        nm = nm.replace('NX', 'untitled_')
    return(nm)

def fix_nx_path(path_str):
    p2 = path_str.split('/')
    s = '/'
    for ss in p2:
        if(len(ss) > 0):
            s += fix_nx_name(ss) + '/'
    return(s)

def print_script_group(dct):
    '''
    '''
    #grp_name = dct['abspath'].replace('/','')
    parent_grp = get_parent_path(dct['abspath'])
    grp_name = get_last_name_from_abspath(dct['abspath'])
    #print('root[\'%s\'] = root.create_group(\'%s\')' % (get_parent_path(dct['abspath']), grp_name))
    if len(parent_grp) > 1:
        #print('root[\'%s\'].create_group(\'%s\')' % (parent_grp, grp_name))
        h5py_script_lst.append(' ')
        h5py_script_lst.append('root[\'%s\'].create_group(\'%s\')' % (parent_grp, grp_name))

        nxsfrmt_script_lst.append('root[\'%s\'] = %s()' % (dct['abspath'], dct['attrib']['type']))

    else:
        #print('root.create_group(\'%s\')' % (grp_name))
        h5py_script_lst.append(' ')
        h5py_script_lst.append('root.create_group(\'%s\')' % (grp_name))
        #h5py_script_lst.append('root[\'%s%s\'] = [()]' % (parent_grp, grp_name))


        nxsfrmt_script_lst.append('root[\'%s\'] = %s()' % (dct['abspath'], dct['attrib']['type']))

def create_groups(nf, dct):
    '''
    walk all dicts of type GROUP creating them as it goes
    '''

    for d in dct['group']:
        _type = get_type(d)
        if('name' in d['attrib'].keys()):
            if _type.replace('NX','') == d['attrib']['name']:
                #use the _type as the name
                #name = _type
                name = d['attrib']['name']
            else:
                name = fix_nx_name(d['attrib']['name'])
        else:
            name = fix_nx_name(_type)

        ppath = fix_nx_path(get_parent_path(d['abspath']))
        if (ppath == ''):
            pgrp = nf
        else:
            pgrp = nf[ppath]
        min_occurs_str = get_min_occurs(d, dct['category'])
        _grp = _group(pgrp, name, nxdata_type=_type, dct=d)
        _string_attr(_grp, 'EX_required', min_occurs_str)
        doc = get_doc(d)
        if (doc):
            _string_attr(_grp, 'doc', doc)
        #print('created: GROUP [%s]' % fix_nx_path(d['abspath']))

def get_dimensions_dicts(d, dct):
    dimensions_dct = next((item for item in dct['dimensions'] if item["abspath"] == d['abspath']), {'attrib':{}})
    #dim_dct = next((item for item in dct['DIM'] if item["abspath"] == d['abspath']), {'attrib':{}})
    dim_l = []
    if('dim' in dct.keys()):
        for item in dct['dim']:
            #get all who share the abspath
            if item['abspath'] == d['abspath']:
                dim_l.append(item['attrib'])

    dimensions_dct['attrib']['dim'] = dim_l
    #dimensions_dct['attrib'].update(dim_dct['attrib'])
    return(dimensions_dct)

def create_fields(nf, dct, sym_dct={}, category=''):
    '''
        walk all dicts of type FIELD creating them as it goes

        {'abspath': '/entry/NXinstrument/fluorescence/data/',
          'xpath': '/definition/group/group[1]/group[3]/field[1]',
          'attrib': {'axes': 'energy',
           'name': 'data',
           'signal': '1',
           'type': 'NX_INT'},
          'doc': '\n     '},
    '''
    global CUR_GRP_DTYPE
    unit_dct = {}
    valid_fld_attr_nms_lst = ['axes','axis','data_offset','interpretation','long_name','maxOccurs','minOccurs','nameType','primary','signal','stride','units'] #type was removed because I think it is handled when dataset is created
    # create FIELDs
    for d in dct['field']:
        name = fix_nx_name(d['attrib']['name'])
        _type = get_type(d)
        units = get_units(_type)
        if len(units) > 0:
            _type = nx_unit_types[units]['type']
            unit_dct = nx_unit_types[units]
        #get teh parent path to this field
        ppath = fix_nx_path(get_parent_path(d['abspath']))

        if len(_type) == 0:
            #there has been no declaration for this item so try to use type of the parent group
            if len(CUR_GRP_DTYPE) == 0:
                #default
                _type = 'NX_CHAR'
            else:
                _type = CUR_GRP_DTYPE
        else:
            #record the type change
            CUR_GRP_DTYPE = _type

        #get all enumerations if any exist for this parent path
        enums = get_enums(d['abspath'], dct)
        #get the dimensions dict if one exists for this field
        if('dimensions' in dct.keys()):
            use_dim_dct_lst = get_dimensions_dicts(d, dct)
        else:
            use_dim_dct_lst = {}
        pgrp = get_parent_group(nf, ppath)
        if len(enums) > 0:
            #if this is an enumerated data type just use teh first enumeration as the data
            data = enums[0]
            # and include a comment in teh script
            print_list_attr(pgrp, name, enums)
        else:
            data = get_nx_data_by_type(_type, use_dim_dct_lst, sym_dct)
            if(data is None):
                print('\t\tError: There is an issue with a non standard field for fieldname [%s]' % name)
                return(False)
        _dset = _dataset(pgrp, name, data, _type, nx_units=units)
        #print('created: FIELD [%s]' % fix_nx_path(d['abspath']))
        _string_attr(_dset, 'EX_required', get_min_occurs(d, category))
        doc = get_doc(d)
        if (doc):
            _string_attr(_dset, 'doc', doc)

        # check for other attributes
        skip_lst = ['minOccurs', 'maxOccurs']
        for a_nm in valid_fld_attr_nms_lst:
            if(a_nm in skip_lst):
                continue
            if a_nm in d['attrib'].keys():
                #check if it already exists
                if a_nm in _dset.attrs.keys():
                    #reassign its value
                    _dset.attrs[a_nm] = d['attrib'][a_nm]
                else:
                    #create it
                    _string_attr(_dset, a_nm, d['attrib'][a_nm])

    return(True)

def create_links(nf, dct):
    '''
    walk all dicts of type LINK creating them as it goes

    '''
    if('link' not in dct.keys()):
        return
    paths_lst = get_all_paths_in_hdf5(nf)
    for d in dct['link']:
        fix_link_target(nf, d, paths_lst)

def fix_link_target(nf, trgt_dct, hdf5_path_lst):
    '''
    pull the actual paths from hdf5 file and make sure the link uses same case sensitivity

    source_addr = u"/entry/instrument/detector/two_theta"   # existing data
    target_addr = u"two_theta"                              # new location
    ds_tth.attrs[u"target"] = source_addr                   # a NeXus API convention for links
    nxdata[target_addr] = f[source_addr]                    # hard link
    # nxdata._id.link(source_addr, target_addr, h5py.h5g.LINK_HARD)

    '''
    ppath = get_parent_path(trgt_dct['abspath'])
    if ppath not in nf:
        print('\t-Error: while checking the links, this parent path [%s] not exist in generated file' % ppath)
        print('\t\ttarget path: [%s] ' % trgt_dct['abspath'])
        exit()
    else:
        pgrp = nf[ppath]
        target_str = trgt_dct['attrib']['target']

        #check target link path against all valid paths in nf file
        for p in hdf5_path_lst:
            # irespective of case is this path a match?
            pstr = '/' + p
            if pstr.casefold() == target_str.casefold():
                # yep  so make sure it uses the valid path case sensitivity
                target_str = p
                break

        if target_str[0] != '/':
            target_str = '/' + target_str

        link_nm = get_last_name_in_path(trgt_dct['abspath'])
        if(target_str not in nf):
            #print('\t-Error: The link path [%s] specified in NXDL file for [%s] does not exist in the generated file' % (target_str, trgt_dct['abspath']))
            print('\t-Link Error: This field [%s] specifies a link target that does not exist in the generated file [%s]' % (trgt_dct['abspath'], target_str))
            #exit()
        else:
            pgrp[link_nm] = nf[target_str]
            nf[target_str].attrs['target'] = target_str


def create_attributes(nf, dct):
    '''
    '''

    if('attribute' not in dct.keys()):
        return
    paths_lst = get_all_paths_in_hdf5(nf)
    for d in dct['attribute']:
        #print(d)
        ppath = get_parent_path(d['abspath'])
        if ppath not in nf:
            print('\t-Error: while creating Attributes, this parent path [%s] not exist in generated file' % ppath)
            exit()
        else:
            pgrp = nf[ppath]

        if('type' in d['attrib'].keys()):
            data = get_nx_data_by_type(d['attrib']['type'])
        else:
            data = get_nx_data_by_type('NX_CHAR')

        #pgrp.attrs[d['attrib']['name']] = data
        if type(data) is str:
            _string_attr(pgrp, d['attrib']['name'], data)
        else:
            _num_attr(pgrp, d['attrib']['name'], data)

def get_last_name_in_path(pth):
    p2 = pth.split('/')
    for p in p2:
        if(len(p) == 0):
            p2.remove(p)
    nm = p2[-1]
    return(nm)

def add_docs(nf, docs):
    '''
    takes a list of dicts that contain doc strings and the paths to the parent field or group, just add the doc string
    as a string attribute to the field or group for the user reference when looking with an hdf5 inspection tool
    '''
    for d in docs:
        # print(d)
        ppath = d['abspath']
        if ppath not in nf:
            #print('\t-Error: while adding doc strings, this parent path [%s] not exist in generated file' % ppath)
            #exit()
            #just skip it
            continue
        else:
            pgrp = nf[ppath]

        _string_attr(pgrp, 'EX_doc', d['doc'].replace('\'', '"'))

def print_script_start(fname):
    print_h5py_ex_start(fname)
    print_nxsfrmt_ex_start(fname)

def print_nxsfrmt_ex_start(fname):
    global nxsfrmt_script_lst
    nxsfrmt_script_lst = []
    nxsfrmt_script_lst.append('from nexusformat.nexus import *')
    nxsfrmt_script_lst.append('# Note this example script was generated by nxdl_to_hdf5.py using the current \n# installed version of the NEXUS definitions ver[%s] ' % rel_ver)
    nxsfrmt_script_lst.append(' ')
    nxsfrmt_script_lst.append('root = NXroot()')


def print_h5py_ex_start(fname):
    global h5py_script_lst
    h5py_script_lst = []
    #fpath = __file__.replace('nxdl_to_hdf5.py','')
    h5py_script_lst.append(' ')
    h5py_script_lst.append('import numpy as np')
    h5py_script_lst.append('import datetime')
    h5py_script_lst.append('import h5py')
    h5py_script_lst.append('import os')
    h5py_script_lst.append(' ')
    h5py_script_lst.append(
        '# Note this example script was generated by nxdl_to_hdf5.py using the current \n# installed version of the NEXUS definitions ver[%s] ' % rel_ver)
    h5py_script_lst.append(' ')
    h5py_script_lst.append('root = h5py.File(\'h5py_%s.h5\', \'w\')' % (fname))

def print_script_close(class_nm):
    print_h5py_close()
    print_nxsfrmt_close(class_nm)

def print_h5py_close():
    #print('root.close()\n\n')
    h5py_script_lst.append('root.close()\n\n')

def print_nxsfrmt_close(class_nm):
    nxsfrmt_script_lst.append('root.save(\'nexusformat_%s.h5\', \'w\')\n\n' % class_nm)

def make_class_as_nf_file(clss_nm, path_dct, dest_dir, docs, symbol_dct = {}):
    print('\texporting: [%s]' % clss_nm)
    res = True
    CUR_GRP_DTYPE = ''
    category = def_dir = path_dct[clss_nm]['category']

    if (not os.path.exists(dest_dir)):
        os.makedirs(dest_dir)

    fpath = os.path.join(dest_dir, '%s.hdf5' % clss_nm)
    nf = h5py.File(fpath, 'w')

    sym_dct = {}
    # process SYMBOLS
    for sym_nm, d in symbol_dct.items():
        if (True):
            #sym_nm = d['name']
            val = int(d['value'])
        else:
            #sym_nm = d['name']
            val = default_symbol_val

        sym_dct[sym_nm] = val

        #create the symbol string attrs in the root of the file
        _string_attr(nf, sym_nm, d['doc'])

        # update the DIMENSIONS list by substituting the value for the symbol where it is used to specify the dimension of
        # a field
        if('dimensions' in path_dct[clss_nm].keys()):
            for l in path_dct[clss_nm]['dimensions']:
                if 'rank' in l['attrib'].keys():
                    if l['attrib']['rank'] == sym_nm:
                        #substitute the value
                        l['attrib']['rank'] = val

        if ('dim' in path_dct[clss_nm].keys()):
            for l in path_dct[clss_nm]['dim']:
                if 'value' in l['attrib'].keys():
                    if l['attrib']['value'] == sym_nm:
                        #substitute the value
                        l['attrib']['value'] = val

    #check if the definition uses a symbol for a value but has not defined that same symbol
    if('dim' in path_dct[clss_nm].keys()):
        for l in path_dct[clss_nm]['dim']:
            if 'value' in l['attrib'].keys():
                val = l['attrib']['value']
                if not has_numbers(val):
                    #does it also contain spaces? if so then it is not a symbol
                    if val.find(' ') == -1:
                        if val not in sym_dct.keys():
                            print('\t-Symbol Warning: the symbol [%s] is being used but has not been defined in the Symbols table, setting to default value of 1' % val)
                            sym_dct[val] = 1
                            l['attrib']['value'] = 1

    print_script_start(class_nm)

    # create GROUPs
    create_groups(nf, path_dct[clss_nm])
    # # create FIELDs
    res = create_fields(nf, path_dct[clss_nm], sym_dct, category)

    # create Links
    create_links(nf, path_dct[clss_nm])

    # add the docs from fields and groups now that they exist
    add_docs(nf, docs)

    if(res):
        # create Attributes
        create_attributes(nf, path_dct[clss_nm])

        _string_attr(nf, 'file_name', fpath.replace('\\', '/'), do_print=False)
        _string_attr(nf, 'file_time', make_timestamp_now(), do_print=False)
        #_string_attr(nf, 'NEXUS_release_ver', rel_ver)
        entry_grp, entry_nm = get_entry(nf)
        #ensure the definition is correct
        entry_grp['definition'][()] = clss_nm
        _string_attr(nf, 'default', entry_nm)
        nx_data_grp, nx_data_nm = get_NXdata_nm(nf)
        if (nx_data_nm):
            _string_attr(entry_grp, 'default', nx_data_nm)
            dset_nm = get_NXdataset_nm(nx_data_grp)
            if (dset_nm):
                _string_attr(nx_data_grp, 'signal', dset_nm)
                _string_attr(nx_data_grp[dset_nm], 'signal', '1')

        _dataset(nf, 'README', readme_string % (rel_ver, clss_nm), 'NX_CHAR', nx_units='NX_UNITLESS', dset={}, do_print=False)
        nf.close()
        #print('finished exporting to [%s]' % fpath)
    else:
        print('Failed exporting [%s]' % fpath)
        nf.close()

    print_script_versions(class_nm)
    print_script_close(class_nm)

    write_script_file(class_nm)
    return(res)

def print_script_versions(fname):
    print_h5py_versions(fname)
    print_nxsfrmt_versions(fname)

def print_h5py_versions(class_nm):
    h5py_script_lst.append('root.attrs[\'file_name\'] = os.path.abspath(\'%s\')' % class_nm)
    h5py_script_lst.append('root.attrs[\'file_time\'] = datetime.datetime.now().isoformat()')
    h5py_script_lst.append('root.attrs[\'h5py_version\'] = h5py.version.version')
    h5py_script_lst.append('root.attrs[\'HDF5_Version\'] = h5py.version.hdf5_version')


def print_nxsfrmt_versions(class_nm):
    nxsfrmt_script_lst.append('root.attrs[\'file_name\'] = os.path.abspath(\'%s\')' % class_nm)
    nxsfrmt_script_lst.append('root.attrs[\'file_time\'] = datetime.datetime.now().isoformat()')
    #nxsfrmt_script_lst.append('root.attrs[\'h5py_version\'] = h5.version.version')
    #nxsfrmt_script_lst.append('root.attrs[\'HDF5_Version\'] = h5.version.hdf5_version')

def write_script_file(class_nm):
    write_h5py_script(os.path.join(os.getcwd(), '..', 'autogenerated_examples','nxdl', 'python_scripts','h5py'), class_nm, h5py_script_lst)
    write_nxsfrmt_script(os.path.join(os.getcwd(), '..', 'autogenerated_examples','nxdl', 'python_scripts', 'nexusformat'), class_nm, nxsfrmt_script_lst)

def write_h5py_script(path, class_nm, script_lst):
        if not os.path.exists(path):
            os.makedirs(path)
        f = open(os.path.join(path, 'ex_h5py_%s.py' % class_nm), 'w')
        for l in script_lst:
            f.write(l + '\n')
        f.close()

def write_nxsfrmt_script(path, class_nm, script_lst):
    if not os.path.exists(path):
        os.mkdir(path)

    f = open(os.path.join(path, 'ex_nexusformat_%s.py' % class_nm), 'w')
    for l in script_lst:
        f.write(l + '\n')
    f.close()

def build_class_dict(class_dir='base_classes', desired_class=None, defdir=None, sym_args_dct={},report_symbols_only=False):
    '''
    build a nxdl definition into a dict
        class_dir: either 'applications' or 'base_classes'
        desired_class: the name of a desired class definition such as 'NXstxm', if left as None then all class definitions\
                will be returned.
        defdir: if the definitions are located somewhere other than in a subdir of nexpy
    '''
    if(defdir is None):
        class_path = pkg_resources.resource_filename('nexpy', 'definitions/%s' % class_dir)
    else:
        class_path = os.path.join(defdir, class_dir)

    nxdl_files = list(map(os.path.basename, glob.glob(os.path.join(class_path, '*.nxdl.xml'))))
    dct = {}
    if(desired_class):
        nxdl_files = [os.path.join(class_path, '%s.nxdl.xml' % desired_class)]

    for nxdl_file in nxdl_files:
        class_nm = nxdl_file.replace('.nxdl.xml', '')
        if(class_nm.find(os.path.sep) > -1):
            class_nm = class_nm.split(os.path.sep)[-1]
        print('\nProcessing [%s]' % nxdl_file)
        resp_dict, syms, docs = get_xml_paths(nxdl_file, sym_args_dct=sym_args_dct, report_symbols_only=report_symbols_only)
        dct[class_nm] = resp_dict
    return(dct, syms, docs)

def symbol_args_to_dict(arg_lst):
    dct = {}
    for arg in arg_lst:
        k,v = arg.replace(',','').strip().split('=')
        dct[k] = {'value': v, 'doc': ''}

    return(dct)

if __name__ == '__main__':
    import argparse

    def_subdirs = ['applications', 'contributed_definitions']

    parser = argparse.ArgumentParser(description="This is a script to generate hdf5 files from nxdl.xml definitions")
    group = parser.add_mutually_exclusive_group()

    group.add_argument("-f","--file",
                        help="The definition file to generate.\nex: python nxdl_to_hdf5.py --f NXstxm")
    group.add_argument("-d","--directory",
                        help="generate all definitions in this directory one of either ['applications', 'contributed_definitions'], ex: applications")
    parser.add_argument("-s","--symbols",
                        help="pass comma delimited set of key value pairs for each desired symbol \nex: python nxdl_to_hdf5.py --f NXstxm --s numP=24, numE=1, numY=10, numX=10", nargs='*')
    parser.add_argument("--nxdefdir", help="Specify an alternative location to the NXDL definitions base directory (where nexpy is installed)")
    parser.add_argument("-r", "--report", help="Report on the Symbols that this definition uses", action="store_true")

    args = parser.parse_args()
    sym_args_dct = {}
    class_nm = None
    class_path = None
    report_symbols_only = False
    if args.file:
        print('\tProcess this specific definition [%s]' % args.file)
        class_nm = args.file
    elif args.directory:
        print('\tProcess this entire directory [%s]' % args.directory)
        def_subdirs = [args.directory]
    else:
        print('\tError: neither a specific definition or directory was specified so nothing to do')
        exit()

    if args.symbols:
        print('\tProcess using the following symbols [%s]' % args.symbols)
        sym_args_dct = symbol_args_to_dict(args.symbols)

    if args.nxdefdir:
        print('\tUsing the following definitions base directory [%s]' % args.nxdefdir)
        def_dir = args.nxdefdir
    else:
        #use the definitions in the installed nexpy
        def_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'definitions')

    #get the release version of the definitions
    if os.path.exists(os.path.join(def_dir, 'NXDL_VERSION')):
        f = open(os.path.join(def_dir, 'NXDL_VERSION'), 'r')
        l = f.readlines()
        f.close()
        rel_ver = l[0].replace('\n','')

    if args.report:
        #just repolrt on the symbols that are defined in
        #only perform a symbol report if the user specified a file
        if(class_nm):
            report_symbols_only = True

    #only search in applications and contributed_definitions subdirectories
    if(class_nm):
        for def_subdir in def_subdirs:
            class_path = os.path.join(def_dir, def_subdir, class_nm + '.nxdl.xml')
            if(os.path.exists(class_path)):
                break
            else:
                class_path = None

        if(class_path is None):
            print('Error: the class name [%s.nxdl.xml] doesnt exist in either of the applications or contributed_definitions subdirectories' % class_nm)
            exit()

    files = None
    for def_subdir in def_subdirs:
        files = sorted(os.listdir(os.path.join(def_dir, def_subdir)))
        dest_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),'..', 'autogenerated_examples', 'nxdl', def_subdir)
        for class_path in files:
            if class_path.find('.nxdl.xml') > -1:
                if(class_nm is None):
                    class_nm = class_path.replace('.nxdl.xml','')
                    path_dct, syms, docs  = build_class_dict(def_subdir, desired_class=class_nm, defdir=def_dir,
                                                       sym_args_dct=sym_args_dct,
                                                       report_symbols_only=report_symbols_only)
                    if(path_dct[class_nm] is None):
                        print('\t[%s] does not contain an NXentry group, skipping' % class_nm)
                        class_nm = None
                        continue

                    res = make_class_as_nf_file(class_nm, path_dct, dest_dir, docs, symbol_dct=sym_args_dct)
                    # if(res):
                    #     run_remote_validator(class_nm)
                    # else:
                    #     print('Not validating the file until NXDL file contains no errors')
                    class_nm = None
                else:
                    path_dct, syms, docs = build_class_dict(def_subdir, desired_class=class_nm,
                                                      defdir=def_dir, sym_args_dct=sym_args_dct,
                                                      report_symbols_only=report_symbols_only)
                    if (path_dct[class_nm] is None):
                        print('\t[%s] does not contain an NXentry group, skipping' % class_nm)
                        class_nm = None
                        continue

                    res = make_class_as_nf_file(class_nm, path_dct, dest_dir, docs, symbol_dct=sym_args_dct)
                    # if (res):
                    #     run_remote_validator(class_nm)
                    # else:
                    #     print('Skipping file validation until [%s] contains no errors' % os.path.join(os.path.join(def_dir, def_subdir), class_nm +'nxdl.xml'))
                    exit()



